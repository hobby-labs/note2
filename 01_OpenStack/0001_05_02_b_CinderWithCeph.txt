= Swift のインストール =
; ceph : BLOCK DEVICES AND OPENSTACK
: https://docs.ceph.com/en/latest/rbd/rbd-openstack/

; RedHat : Chapter 3. Configuring OpenStack to Use Ceph
: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/ceph_block_device_to_openstack_guide/configuring_openstack_to_use_ceph

; Configuring the Block (cinder) storage service (optional)
: https://docs.openstack.org/openstack-ansible-os_cinder/zed/configure-cinder.html

; Ceph Quincy : Configure Cluster #1
: https://www.server-world.info/en/note?os=Ubuntu_22.04&p=ceph&f=1

Cinder のバックエンドとしてCeph を使う設定について説明していきます。
ここでは、下記の3 ノードをCeph クラスタとして使っていきます。
下記と同様の定義を、各ノードの/etc/hosts に設定してください。

* /etc/hosts @ dev-storage01, dev-storage02, dev-storage03
<syntaxhighlight lang="text">
172.22.1.101 dev-storage01 dev-storage01.openstack.example.com
172.22.1.102 dev-storage02 dev-storage02.openstack.example.com
172.22.1.103 dev-storage03 dev-storage03.openstack.example.com
</syntaxhighlight>


== Ceph バックエンド ==
OpenStack のCinder では、Ceph のブロックデバイスをバックエンドとして、取り付けることができます。
Ceph はブロックボリュームを複数のOSD(Object Storage Daemon) に分散できるため、高い性能が期待できます。<br /><br />

Ceph をバックエンドとして利用するために、<code>libvirtd</code> をインストールする必要があります。<br /><br />

OpenStack はCeph をバックエンドにすることでImage サービス、Volume(Cinder)、ゲストディスクを統合することができますが、ここではVolume(Cinder)について、説明していきます。

== Ceph のSSH 認証設定 ==
Ceph cluster 内で、使用するSSH 公開鍵を作成します。
代表して、dev-storage01 で作成します。

* dev-storage01
<syntaxhighlight lang="console">
dev-storage01 # ssh-keygen -q -N "" -f ~/.ssh/ceph_cluster
dev-storage01 # cp ~/.ssh/ceph_cluster.pub ~/.ssh/authorized_keys
dev-storage01 # chmod 600 ~/.ssh/*
</syntaxhighlight>

<code>ceph_cluster</code>(秘密鍵), <code>ceph_cluster.pub</code>(公開鍵), <code>authorized_keys</code> ファイルを、残りのノード<code>dev-storage02</code>, <code>dev-storage03</code> にコピーし、権限を<code>600</code>に設定してください。<br /><br />

<!--
# Commands in workspace. It is an additional comment for my environment.
worksptation# for target in dev-storage02 dev-storage03 dev-compute01 dev-compute02; do
    echo "target => ${target}"
    ssh dev-storage01 -- sudo cat /root/.ssh/ceph_cluster | ssh ${target} -- sudo bash -c "cat - | sudo tee /root/.ssh/ceph_cluster > /dev/null"
    ssh dev-storage01 -- sudo cat /root/.ssh/ceph_cluster.pub | ssh ${target} -- sudo bash -c "cat - | sudo tee /root/.ssh/ceph_cluster.pub > /dev/null"
    ssh dev-storage01 -- sudo cat /root/.ssh/authorized_keys | ssh ${target} -- sudo bash -c "cat - | sudo tee /root/.ssh/authorized_keys > /dev/null"
    ssh ${target} -- sudo chmod 600 /root/.ssh/{ceph_cluster,ceph_cluster.pub,authorized_keys}
done
-->

次にSSH クライアント設定ファイルを作成します。

* ~/.ssh/config @ dev-storage01
<syntaxhighlight lang="console">
dev-storage01 # cat << EOF > ~/.ssh/config
Host dev-storage* dev-storage*.openstack.example.com dev-compute* dev-compute*.openstack.example.com
    PreferredAuthentications publickey
    User root
    IdentityFile ~/.ssh/ceph_cluster
EOF

dev-storage01 # chmod 600 ~/.ssh/config
</syntaxhighlight>

この情報を各ノードにコピーします。

<syntaxhighlight lang="console">
dev-storage01 # for node in dev-storage02 dev-storage03 dev-compute01 dev-compute02; do
    scp -i ~/.ssh/ceph_cluster ~/.ssh/config ${node}:.ssh/config
    ssh -i ~/.ssh/ceph_cluster ${node} -- chmod 600 .ssh/config
done
</syntaxhighlight>

// Snapshot created_ssh_keys

== Ceph をインストールする ==

* dev-storage01
<syntaxhighlight lang="console">
dev-storage01 # for node in dev-storage01 dev-storage02 dev-storage03
do
    ssh ${node} "apt update; apt -y install ceph"
done
</syntaxhighlight>

== 管理ノードでMonitor daemon とManager daemon を設定する ==

<syntaxhighlight lang="console">
dev-storage01 # uuidgen
ffffffff-ffff-ffff-ffff-ffffffffffff
</syntaxhighlight>

クラスタ設定ファイルを作成します。
ファイル名は<code>(Cluster name).confg</code> となるように設定します。

* /etc/ceph/ceph.conf @ dev-storage01
<syntaxhighlight lang="text">
[global]
# specify cluster network for monitoring
cluster network = 172.22.0.0/16
# specify public network
public network = 172.22.0.0/16

# specify UUID genarated above
fsid = ffffffff-ffff-ffff-ffff-ffffffffffff
# specify IP address of Monitor Daemon
mon host = 172.22.1.101
# specify Hostname of Monitor Daemon
mon initial members = dev-storage01
osd pool default crush rule = -1

# mon.(Node name)
[mon.dev-storage01]
# specify Hostname of Monitor Daemon
host = dev-storage01
# specify IP address of Monitor Daemon
mon addr = 172.22.1.101
# allow to delete pools
mon allow pool delete = true
</syntaxhighlight>

クラスタの監視のための秘密鍵を生成します。

* dev-storage01
<syntaxhighlight lang="console">
dev-storage01 # ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
</syntaxhighlight>

クラスタ管理のための秘密鍵を作成します。

* dev-storage01
<syntaxhighlight lang="console">
dev-storage01 # ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
</syntaxhighlight>

起動のための秘密鍵を作成します。
<syntaxhighlight lang="console">
dev-storage01 # ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'
</syntaxhighlight>

作成された鍵をインポートします。
<syntaxhighlight lang="console">
dev-storage01 # ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
dev-storage01 # ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
</syntaxhighlight>

// Snapshot imported_generated_keys

== モニタマップを作成する ==
<syntaxhighlight lang="console">
dev-storage01 # FSID=$(grep "^fsid" /etc/ceph/ceph.conf | awk {'print $NF'})
dev-storage01 # NODENAME=$(grep "^mon initial" /etc/ceph/ceph.conf | awk {'print $NF'})
dev-storage01 # NODEIP=$(grep "^mon host" /etc/ceph/ceph.conf | awk {'print $NF'})
dev-storage01 # echo "Debug. FSID=${FSID}, NODENAME=${NODENAME}, NODEIP=${NODEIP}"
dev-storage01 # monmaptool --create --add $NODENAME $NODEIP --fsid $FSID /etc/ceph/monmap
</syntaxhighlight>

モニタデーモンのためのディレクトリを作成します。
ディレクトリ名は<code>/var/lib/ceph/mon/<Cluster name>-<Host name></code> となるようにしてください。

<syntaxhighlight lang="console">
dev-storage01 # mkdir /var/lib/ceph/mon/ceph-${NODENAME}
</syntaxhighlight>

鍵とモニタマップをモニタデーモンに関連付けます。
<code>--cluster</code> オプションにはクラスタ名を指定してください。

<syntaxhighlight lang="console">
dev-storage01 # ceph-mon --cluster ceph --mkfs -i $NODENAME --monmap /etc/ceph/monmap --keyring /etc/ceph/ceph.mon.keyring
dev-storage01 # chown ceph. /etc/ceph/ceph.*
dev-storage01 # chown -R ceph. /var/lib/ceph/mon/ceph-${NODENAME} /var/lib/ceph/bootstrap-osd
dev-storage01 # systemctl enable --now ceph-mon@${NODENAME}
</syntaxhighlight>

Messenger v2 プロトコルを有効化します。
<syntaxhighlight lang="console">
dev-storage01 # ceph mon enable-msgr2
dev-storage01 # ceph config set mon auth_allow_insecure_global_id_reclaim false
</syntaxhighlight>

Placement Groups オートスケールモジュールを有効化します。
<syntaxhighlight lang="console">
dev-storage01 # ceph mgr module enable pg_autoscaler
</syntaxhighlight>

管理デーモンのディレクトリを作成します。

<syntaxhighlight lang="console">
dev-storage01 # mkdir /var/lib/ceph/mgr/ceph-${NODENAME}
</syntaxhighlight>

認証用の鍵を作成します。

<syntaxhighlight lang="console">
dev-storage01 # ceph auth get-or-create mgr.${NODENAME} mon 'allow profile mgr' osd 'allow *' mds 'allow *'
[mgr.dev-storage01]
        key = ..................................

dev-storage01 # ceph auth get-or-create mgr.${NODENAME} | tee /etc/ceph/ceph.mgr.admin.keyring
dev-storage01 # cp /etc/ceph/ceph.mgr.admin.keyring /var/lib/ceph/mgr/ceph-${NODENAME}/keyring
dev-storage01 # chown ceph. /etc/ceph/ceph.mgr.admin.keyring
dev-storage01 # chown -R ceph. /var/lib/ceph/mgr/ceph-${NODENAME}
dev-storage01 # systemctl enable --now ceph-mgr@${NODENAME}
</syntaxhighlight>

<syntaxhighlight lang="console">
dev-storage01 # ceph -s
  cluster:
    id:     ffffffff-ffff-ffff-ffff-ffffffffffff
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum dev-storage01 (age 12m)
    mgr: dev-storage01(active, since 12s)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
</syntaxhighlight>

// Snapshot configure_ceph_manager_mon_node

== クラスタ(OSD:Object Storage Device)を設定する ==
; Ceph Quincy : Configure Cluster #2
: https://www.server-world.info/en/note?os=Ubuntu_22.04&p=ceph&f=2

管理ノードから、各ノードの<code>/dev/sdb</code> をOSD のボリュームとしてフォーマットして使います。
下記コマンドをターミナルに貼り付けて、各ノードのCephボリュームを作成します。

* dev-storage01
<syntaxhighlight lang="text">
for node in dev-storage01 dev-storage02 dev-storage03
do
    if [ ! ${node} = "dev-storage01" ]
    then
        scp /etc/ceph/ceph.conf ${node}:/etc/ceph/ceph.conf
        scp /etc/ceph/ceph.client.admin.keyring ${node}:/etc/ceph
        scp /var/lib/ceph/bootstrap-osd/ceph.keyring ${node}:/var/lib/ceph/bootstrap-osd
    fi
    ssh $node \
        "chown ceph. /etc/ceph/ceph.* /var/lib/ceph/bootstrap-osd/*; \
        parted --script /dev/vdb 'mklabel gpt'; \
        parted --script /dev/vdb "mkpart primary 0% 100%"; \
        count=0; \
        while [ \${count} -lt 30 ]; \
        do \
            echo \"\$(date) - INFO: Creating a ceph volume at /dev/vdb1 on \$(uname -n)\"; \
            ceph-volume lvm create --data /dev/vdb1; \
            vg_name=\$(vgdisplay | grep -P \"^ *VG Name *ceph\-.*\$\" | grep -o '[^ ]*\$'); \
            if [[ \"\${vg_name}\" =~ ^ceph\\-.*\$ ]]; \
            then \
                echo \"\$(date) - INFO: Volume group for Ceph has found. vg_name=\${vg_name}.\"; \
                break; \
            fi; \
            (( ++count )); \
            echo \"\$(date) - ERROR: Failed to create ceph volume. Retrying to execute it agin (count=\${count}).\" >&2; \
            sleep 5; \
        done;"
done
</syntaxhighlight>

クラスタの状態を確認します。

<syntaxhighlight lang="console">
dev-storage01 # ceph -s
  cluster:
    id:     ffffffff-ffff-ffff-ffff-ffffffffffff
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum dev-storage01 (age 45m)
    mgr: dev-storage01(active, since 45m)
    osd: 3 osds: 3 up (since 10m), 3 in (since 10m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

dev-storage01 # ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME               STATUS  REWEIGHT  PRI-AFF
-1         0.02339  root default
-3         0.00780      host dev-storage01
 0    hdd  0.00780          osd.0               up   1.00000  1.00000
-5         0.00780      host dev-storage02
 1    hdd  0.00780          osd.1               up   1.00000  1.00000
-7         0.00780      host dev-storage03
 2    hdd  0.00780          osd.2               up   1.00000  1.00000

dev-storage01 # ceph df
--- RAW STORAGE ---
CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
hdd    24 GiB  24 GiB  61 MiB    61 MiB       0.25
TOTAL  24 GiB  24 GiB  61 MiB    61 MiB       0.25

--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  389 KiB        2  1.1 MiB      0    7.6 GiB

dev-storage01 # ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META    AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.00780   1.00000  8.0 GiB   20 MiB  568 KiB   0 B  20 MiB  8.0 GiB  0.25  1.00    1      up
 1    hdd  0.00780   1.00000  8.0 GiB   20 MiB  568 KiB   0 B  20 MiB  8.0 GiB  0.25  1.00    1      up
 2    hdd  0.00780   1.00000  8.0 GiB   20 MiB  564 KiB   0 B  20 MiB  8.0 GiB  0.25  1.00    1      up
                       TOTAL   24 GiB   61 MiB  1.7 MiB   0 B  59 MiB   24 GiB  0.25
MIN/MAX VAR: 1.00/1.00  STDDEV: 0
</syntaxhighlight>

// Snapshot created_ceph_cluster

== Block デバイスを使用する ==
; Use Block Device
: https://www.server-world.info/en/note?os=Ubuntu_22.04&p=ceph&f=3

クライアント側にssh の公開鍵を送信します。
ここでは、<code>dev-storage01</code> で作成された公開鍵を<code>dev-compute01</code>, <code>dev-compute02</code> にコピーします。
手順は割愛します。<br /><br />

次に、各クライアントに<code>ceph-common</code> をインストールし、設定していきます。

<syntaxhighlight lang="console">
dev-storage01 # for node in dev-compute01 dev-compute02; do
    ssh ${node} "apt -y install ceph-common"
    scp /etc/ceph/ceph.conf ${node}:/etc/ceph/

    scp /etc/ceph/ceph.client.admin.keyring ${node}:/etc/ceph/
    ssh ${node} "chown ceph. /etc/ceph/ceph.*"
done
</syntaxhighlight>

== クライアントでのデバイス作成とマウント ==
ここから先は、Ceph クライアントノードで作業を実施していきます。
クライアント側でデバイスを作成し、マウントするために、まずプールを作成します。
ここでは<code>rbd</code> プールを作成します。

<syntaxhighlight lang="console">
dev-storage01 # ceph osd pool create rbd 32
</syntaxhighlight>

Placement グループのオートスケールを有効化します。

<syntaxhighlight lang="console">
// dev-compute{01, 02}
dev-storage01 # ceph osd pool set rbd pg_autoscale_mode on
</syntaxhighlight>

オートスケールの初期化と、状態の確認をします。

<syntaxhighlight lang="console">
// dev-compute{01,02}
dev-storage01 # rbd pool init rbd

// dev-compute{01,02}
dev-storage01 # ceph osd pool autoscale-status
</syntaxhighlight>

1GB のブロックデバイスを作成します。

<syntaxhighlight lang="console">
// dev-compute{01,02}
dev-storage01 # rbd create --size 1GB --pool rbd rbd01

// dev-compute{01,02}
dev-storage01 # rbd ls -l
NAME   SIZE   PARENT  FMT  PROT  LOCK
rbd01  1 GiB            2

</syntaxhighlight>

デバイスをマッピングします。
クライアントノードで下記コマンドを実行し、クライアントのドライブにマッピングします。
<syntaxhighlight lang="console">
dev-compute01 # rbd map rbd01
// 1 デバイス1 クライアントでマッピングします。1 デバイス複数クライアントでマッピング/マウントして使用しても、正常に利用できません

dev-compute01 # rbd showmapped
id  pool  namespace  image  snap  device
0   rbd              rbd01  -     /dev/rbd0
</syntaxhighlight>

ブロックデバイスをフォーマットします。

<syntaxhighlight lang="console">
dev-compute01 # mkfs.xfs /dev/rbd0
...
dev-compute01 # mount /dev/rbd0 /mnt
</syntaxhighlight>

// Snapshot mount_device_on_client

== ブロックデバイス、Pool の削除 ==
デバイスのunmap をするには、<code>rbd unmap</code> コマンドを実行します。

<syntaxhighlight lang="console">
dev-compute01 # rbd unmap /dev/rbd/rbd/rbd01
</syntaxhighlight>

Block デバイスを削除する。

<syntaxhighlight lang="console">
dev-compute01 # rbd rm rbd01 -p rbd
</syntaxhighlight>

<syntaxhighlight lang="console">
dev-compute01 # ceph osd pool delete rbd rbd --yes-i-really-really-mean-it
</syntaxhighlight>

== ファイルシステムを使用する ==
; Use File System
: https://www.server-world.info/en/note?os=Ubuntu_22.04&p=ceph&f=4

; Introduction to Ceph
: https://parhamzardoshti.medium.com/introduction-to-ceph-7ed07be08a69

SSH の鍵の転送については、手順を割愛します。<br /><br />

ノード上のMetaData Server を設定します。
まず、ディレクトリを作成します。
ディレクトリ名はクラスタ名とノード名を含む<code>directory name ⇒ (Cluster Name)-(Node Name)</code> となるようにしてください。

<syntaxhighlight lang="console">
dev-storage01 # mkdir -p /var/lib/ceph/mds/ceph-${HOSTNAME}
dev-storage01 # ceph-authtool --create-keyring /var/lib/ceph/mds/ceph-${HOSTNAME}/keyring --gen-key -n mds.${HOSTNAME}
creating ...
dev-storage01 # chown -R ceph. /var/lib/ceph/mds/ceph-${HOSTNAME}
dev-storage01 # ceph auth add mds.${HOSTNAME} osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/ceph-${HOSTNAME}/keyring
added key for mds.${HOSTNAME}
dev-storage01 # systemctl enable --now ceph-mds@${HOSTNAME}
</syntaxhighlight>

MDS ノード上に2 つのRADOS プールを作成します。

<syntaxhighlight lang="console">
dev-storage01 # # 下記のCeph プールを作成するコマンドの数字は、下記ドキュメントを参照してください
dev-storage01 # # http://docs.ceph.com/docs/master/rados/operations/placement-groups/
dev-storage01 # ceph osd pool create cephfs_data 32

dev-storage01 # ceph osd pool create cephfs_data 32

</syntaxhighlight>

